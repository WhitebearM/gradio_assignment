{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import speech_recognition as sr\n",
    "import pymysql\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from wordcloud import WordCloud\n",
    "from pydub import AudioSegment\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from deep_translator import GoogleTranslator\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 초기화\n",
    "model = ChatOllama(model=\"gemma2\", temperature= 0.7, verbose= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#번역 함수\n",
    "def translate_to_korean(text):\n",
    "    translated = GoogleTranslator(source='en', target=\"ko\").translate(text)\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 설명 생성 함수\n",
    "def generate_image_description(image):\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image(image):\n",
    "    \"\"\"\n",
    "    이미지를 입력받아 설명을 생성하고 한국어로 번역하는 함수\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return \"이미지를 업로드해주세요.\"\n",
    "    \n",
    "    try:\n",
    "        # 이미지 설명 생성\n",
    "        description = generate_image_description(image)\n",
    "        \n",
    "        # 설명을 한국어로 번역\n",
    "        translated_description = translate_to_korean(description)\n",
    "        \n",
    "        return translated_description\n",
    "    except Exception as e:\n",
    "        return f\"이미지 분석에 실패했습니다: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#챗봇\n",
    "def chat(message, history):\n",
    "    chat_history = []\n",
    "    for human, ai in history:\n",
    "        chat_history.append(HumanMessage(content=human))\n",
    "        chat_history.append(AIMessage(content=ai))\n",
    "    \n",
    "    if message:\n",
    "        chat_history.append(HumanMessage(content=message))\n",
    "        response = model.invoke(chat_history)\n",
    "        \n",
    "             # 반환값이 AIMessage인 경우 문자열로 변환\n",
    "        if hasattr(response, \"content\"):\n",
    "            return response.content  # 문자열 반환\n",
    "        else:\n",
    "            return str(response)  # 예상치 못한 경우 문자열로 변환\n",
    "    \n",
    "    return \"메시지를 입력해주세요.\"\n",
    "        \n",
    "    # chat_history.append(HumanMessage(content=message))\n",
    "    # response = model.invoke(chat_history)\n",
    "        \n",
    "    # return response\n",
    "    #위코드는 반환되면서 객체로 반환 그래서 에러나는거였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(uploaded_file):\n",
    "    if uploaded_file is not None:\n",
    "        try:\n",
    "            file_path = uploaded_file.name\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "        except Exception as e:\n",
    "            return f\"파일 처리 중 오류: {e}\", None\n",
    "    else:\n",
    "        return \"텍스트 파일을 업로드해주세요.\", None\n",
    "\n",
    "    if not text.strip():\n",
    "        return \"텍스트 파일이 비어 있습니다.\", None\n",
    "\n",
    "    # 워드 클라우드 생성\n",
    "    wordcloud = WordCloud(\n",
    "        font_path='C:/Windows/Fonts/malgun.ttf',  # 한글 폰트 경로\n",
    "        background_color='white',\n",
    "        width=800,\n",
    "        height=600,\n",
    "        max_words=200\n",
    "    ).generate(text)\n",
    "\n",
    "    # 이미지를 로컬 파일로 저장\n",
    "    output_path = \"wordcloud.png\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    # os.remove(output_path)\n",
    "\n",
    "    return \"워드 클라우드가 성공적으로 생성되었습니다.\", output_path\n",
    "\n",
    "def clean_up_image(image_path):\n",
    "    \"\"\"이미지 파일 삭제\"\"\"\n",
    "    if os.path.exists(image_path):\n",
    "        os.remove(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(uploaded_audio):\n",
    "    if uploaded_audio is None:\n",
    "        return \"오디오 파일을 업로드해주세요.\"\n",
    "\n",
    "    # SpeechRecognition을 사용하여 음성 텍스트 변환\n",
    "    recognizer = sr.Recognizer()\n",
    "    try:\n",
    "        # 업로드된 오디오 파일 경로 가져오기\n",
    "        audio_path = uploaded_audio.name\n",
    "     \n",
    "             # WAV로 변환\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "        wav_path = \"temp.wav\"\n",
    "        audio.export(wav_path, format=\"wav\")\n",
    "        print(\"[DEBUG] 파일이 WAV 형식으로 변환되었습니다.\")\n",
    "\n",
    "        # SpeechRecognition으로 변환\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.AudioFile(wav_path) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "            text = recognizer.recognize_google(audio_data, language=\"ko-KR\")\n",
    "        \n",
    "        # 변환 후 임시 파일 삭제\n",
    "        os.remove(wav_path)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        return \"음성을 인식할 수 없습니다.\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"STT 서비스에 문제가 발생했습니다: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #mysql DB연결 설정\n",
    "# def connect_to_db():\n",
    "#     connection = pymysql.connect(\n",
    "#         host=\"localhost\",\n",
    "#         user=\"root\",\n",
    "#         password=\"1234\",\n",
    "#         database=\"test\",\n",
    "#         charset=\"utf8mb4\",\n",
    "#         cursorclass=pymysql.cursors.DictCursor  # 결과를 딕셔너리 형태로 반환\n",
    "#     )\n",
    "#     return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def execute_query(query):\n",
    "#     connection = connect_to_db()\n",
    "#     try:\n",
    "#         with connection.cursor() as cursor:\n",
    "#             cursor.execute(query)\n",
    "#             results = cursor.fetchall()\n",
    "#         return results\n",
    "#     finally:\n",
    "#         connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task text2sql, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 허깅페이스 text-to-sql 모델 로드\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m text_to_sql \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext2sql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalesforce/grappa_large_jnt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnatural_language_to_sql\u001b[39m(natural_language, table_schema):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#자연어 질문을 sql 쿼리로 변환\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 모델에 테이블 스키마와 질문 제공\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m    테이블 스키마: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_schema\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124m    질문: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnatural_language\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[1;32mc:\\gradio_assignment\\.venv\\lib\\site-packages\\transformers\\pipelines\\__init__.py:893\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    886\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[0;32m    887\u001b[0m             class_ref,\n\u001b[0;32m    888\u001b[0m             model,\n\u001b[0;32m    889\u001b[0m             code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[0;32m    890\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    891\u001b[0m         )\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 893\u001b[0m     normalized_task, targeted_task, task_options \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pipeline_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\gradio_assignment\\.venv\\lib\\site-packages\\transformers\\pipelines\\__init__.py:548\u001b[0m, in \u001b[0;36mcheck_task\u001b[1;34m(task)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_task\u001b[39m(task: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, Dict, Any]:\n\u001b[0;32m    504\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;124;03m    Checks an incoming task string, to validate it's correct and return the default Pipeline and Model classes, and\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;124;03m    default models if they exist.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIPELINE_REGISTRY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\gradio_assignment\\.venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1444\u001b[0m, in \u001b[0;36mPipelineRegistry.check_task\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m task, targeted_task, (tokens[\u001b[38;5;241m1\u001b[39m], tokens[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid translation task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1444\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, available tasks are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_supported_tasks()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1446\u001b[0m )\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Unknown task text2sql, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "# # 허깅페이스 text-to-sql 모델 로드\n",
    "# text_to_sql = pipeline(\"text2sql\", model=\"microsoft/table-transformer-text-to-sql\")\n",
    "\n",
    "# def natural_language_to_sql(natural_language, table_schema):\n",
    "#     #자연어 질문을 sql 쿼리로 변환\n",
    "#     # 모델에 테이블 스키마와 질문 제공\n",
    "#     input_text = f\"\"\"\n",
    "#     테이블 스키마: {table_schema}\n",
    "#     질문: \"{natural_language}\"\n",
    "#     \"\"\"\n",
    "#     result = text_to_sql(input_text)\n",
    "#     return result[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\gradio_assignment\\.venv\\lib\\site-packages\\gradio\\components\\chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#탭공간\n",
    "with gr.Blocks() as iface:\n",
    "    with gr.Tab(\"무물보\"):\n",
    "        demo = gr.ChatInterface(\n",
    "            fn = chat,\n",
    "            examples=[\n",
    "                \"안녕하세요!\",\n",
    "                \"오늘의 저녁을 추천해주세요\",\n",
    "                \"대한민국 인원은 몇명인가요?\"\n",
    "            ],\n",
    "            title = \"AI챗봇\",\n",
    "            description=\"무엇이든 물어보세요!\"\n",
    "        )\n",
    "        \n",
    "    with gr.Tab(\"이미지 분석\"):\n",
    "        with gr.Row():\n",
    "            image_input = gr.Image(type=\"pil\", label=\"이미지를 업로드해주세요\")\n",
    "            \n",
    "        submit_button = gr.Button(\"분석\")\n",
    "        output_text = gr.Textbox(lines=5, label=\"결과\")\n",
    "        \n",
    "        submit_button.click(\n",
    "            fn=analyze_image,\n",
    "            inputs=[image_input],\n",
    "            outputs=output_text,\n",
    "        )\n",
    "        \n",
    "    with gr.Tab(\"wordCloud\"):\n",
    "        gr.Markdown(\"텍스트를 입력하거나 파일을 업로드 해주세요\")\n",
    "        \n",
    "        upload_file = gr.File(label=\"파일 업로드\", file_types=['.txt'])\n",
    "        output_image = gr.Image(label=\"워드 클라우드 결과\")\n",
    "        output_massage = gr.Textbox(label=\"결과 메세지\", interactive=False)\n",
    "        generate_btn = gr.Button(\"결과 생성\")\n",
    "    \n",
    "        generate_btn.click(\n",
    "            fn=generate_wordcloud,\n",
    "            inputs=[upload_file],\n",
    "            outputs=[output_massage,output_image],\n",
    "        )\n",
    "        \n",
    "        generate_btn.click(\n",
    "            fn=lambda upload_file: clean_up_image(\"wordcloud.png\"),\n",
    "            inputs=[upload_file],\n",
    "            outputs=[],\n",
    "        )\n",
    "        \n",
    "    with gr.Tab(\"음성 텍스트 반환\"):\n",
    "        gr.Markdown(\"MP3파일을 업로드 해주세요\")\n",
    "        audio_file = gr.File(label=\"오디오 파일 업로드\", file_types=[\".wav\",\".mp3\"])\n",
    "        output_text = gr.Textbox(label=\"변환된 텍스트\")\n",
    "        transcribe_btn = gr.Button(\"반환 시작\")\n",
    "        \n",
    "        transcribe_btn.click(\n",
    "            fn = transcribe_audio,\n",
    "            inputs= [audio_file],\n",
    "            outputs=[output_text]\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7989\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7989/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface.launch(server_port=7989, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 파일이 WAV 형식으로 변환되었습니다.\n",
      "[DEBUG] 파일이 WAV 형식으로 변환되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# iface.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
